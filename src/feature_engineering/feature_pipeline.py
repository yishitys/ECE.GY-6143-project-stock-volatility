"""
ç‰¹å¾å·¥ç¨‹ä¸»æµç¨‹

æ•´åˆæ‰€æœ‰ç‰¹å¾å·¥ç¨‹æ­¥éª¤ï¼š
1. åŠ è½½åˆå¹¶æ•°æ®
2. è®¡ç®—ç›®æ ‡å˜é‡
3. ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼ˆå¦‚æœå°šæœªç”Ÿæˆï¼‰
4. èšåˆæ–‡æœ¬ç‰¹å¾
5. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
6. åˆå¹¶æ‰€æœ‰ç‰¹å¾
7. å¤„ç†ç¼ºå¤±å€¼
8. ä¿å­˜æœ€ç»ˆç‰¹å¾æ•°æ®é›†
"""

import pandas as pd
import numpy as np
import logging
import os
from pathlib import Path
from typing import Optional, Dict, List
import sys

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
src_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

from feature_engineering.calculate_target import (
    calculate_multiple_targets,
    analyze_target_distribution,
    save_target_analysis_report
)
from feature_engineering.generate_embeddings import (
    generate_embeddings_for_reddit_data,
    EmbeddingGenerator
)
from feature_engineering.aggregate_features import (
    aggregate_embeddings_by_hour,
    combine_aggregated_features,
    handle_missing_hours
)
from feature_engineering.technical_indicators import (
    calculate_technical_indicators
)
from data_loading.load_reddit_data import load_multiple_subreddits
from preprocessing.clean_text import clean_reddit_data

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_merged_data(merged_data_path: str) -> pd.DataFrame:
    """
    åŠ è½½åˆå¹¶åçš„æ•°æ®
    
    Args:
        merged_data_path: åˆå¹¶æ•°æ®æ–‡ä»¶è·¯å¾„
    
    Returns:
        åˆå¹¶æ•°æ®DataFrame
    """
    if not os.path.exists(merged_data_path):
        raise FileNotFoundError(f"åˆå¹¶æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {merged_data_path}")
    
    logger.info(f"åŠ è½½åˆå¹¶æ•°æ®: {merged_data_path}")
    df = pd.read_csv(merged_data_path, parse_dates=['timestamp'])
    logger.info(f"åŠ è½½äº† {len(df)} æ¡è®°å½•")
    
    return df


def load_reddit_posts_for_embeddings(
    stock_symbol: str,
    subreddits: List[str] = None,
    reddit_data_dir: str = 'data/raw',
    cleaned_reddit_path: str = 'data/processed/reddit_cleaned.csv',
    start_date: str = '2021-01-01',
    end_date: str = '2021-12-31'
) -> pd.DataFrame:
    """
    åŠ è½½Redditå¸–å­æ•°æ®ç”¨äºç”ŸæˆåµŒå…¥
    
    Args:
        stock_symbol: è‚¡ç¥¨ä»£ç 
        subreddits: å­ç‰ˆå—åˆ—è¡¨
        reddit_data_dir: Redditæ•°æ®ç›®å½•
        cleaned_reddit_path: æ¸…æ´—åçš„Redditæ•°æ®è·¯å¾„
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    
    Returns:
        Redditå¸–å­DataFrame
    """
    # å°è¯•åŠ è½½å·²æ¸…æ´—çš„æ•°æ®
    if os.path.exists(cleaned_reddit_path):
        logger.info(f"åŠ è½½å·²æ¸…æ´—çš„Redditæ•°æ®: {cleaned_reddit_path}")
        df = pd.read_csv(cleaned_reddit_path, parse_dates=['timestamp'], low_memory=False)
        logger.info(f"åŠ è½½äº† {len(df)} æ¡å¸–å­")
        return df
    
    # å¦åˆ™ä»åŸå§‹æ•°æ®åŠ è½½
    logger.info("ä»åŸå§‹æ•°æ®åŠ è½½Redditå¸–å­...")
    if subreddits is None:
        subreddits = [
            'stocks', 'wallstreetbets', 'investing', 'stockmarket',
            'options', 'pennystocks', 'gme'
        ]
    
    df = load_multiple_subreddits(
        subreddits=subreddits,
        data_dir=reddit_data_dir,
        start_date=start_date,
        end_date=end_date,
        prefer_h5=True
    )
    
    if df.empty:
        raise ValueError("æ— æ³•åŠ è½½Redditæ•°æ®")
    
    # æ¸…æ´—æ–‡æœ¬
    df = clean_reddit_data(df, text_column='text_content')
    
    return df


def build_feature_pipeline(
    stock_symbol: str,
    merged_data_path: str = None,
    reddit_data_dir: str = 'data/raw',
    output_dir: str = 'data/processed',
    embedding_model: str = 'all-MiniLM-L6-v2',
    aggregation_method: str = 'mean',
    generate_embeddings: bool = True,
    use_cached_embeddings: bool = True
) -> pd.DataFrame:
    """
    æ„å»ºå®Œæ•´çš„ç‰¹å¾å·¥ç¨‹æµç¨‹
    
    Args:
        stock_symbol: è‚¡ç¥¨ä»£ç 
        merged_data_path: åˆå¹¶æ•°æ®è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
        reddit_data_dir: Redditæ•°æ®ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        embedding_model: åµŒå…¥æ¨¡å‹åç§°
        aggregation_method: èšåˆæ–¹æ³• ('mean', 'weighted_mean', 'max')
        generate_embeddings: æ˜¯å¦ç”ŸæˆåµŒå…¥ï¼ˆå¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼‰
        use_cached_embeddings: æ˜¯å¦ä½¿ç”¨ç¼“å­˜çš„åµŒå…¥
    
    Returns:
        åŒ…å«æ‰€æœ‰ç‰¹å¾çš„DataFrame
    """
    logger.info("="*60)
    logger.info(f"å¼€å§‹ç‰¹å¾å·¥ç¨‹æµç¨‹: {stock_symbol}")
    logger.info("="*60)
    
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Step 1: åŠ è½½åˆå¹¶æ•°æ®
    if merged_data_path is None:
        merged_data_path = os.path.join(output_dir, f'merged_data_{stock_symbol}.csv')
    
    df = load_merged_data(merged_data_path)
    
    # Step 2: è®¡ç®—ç›®æ ‡å˜é‡
    logger.info("\n" + "="*60)
    logger.info("Step 2: è®¡ç®—ç›®æ ‡å˜é‡")
    logger.info("="*60)
    df = calculate_multiple_targets(df, methods=['log_return_abs', 'price_range'])
    
    # åˆ†æç›®æ ‡å˜é‡
    target_col = 'target_volatility_log_return_abs'
    if target_col in df.columns:
        stats = analyze_target_distribution(df, target_col)
        report_path = os.path.join(output_dir, f'target_analysis_{stock_symbol}.md')
        save_target_analysis_report(stats, report_path, target_col)
    
    # Step 3: ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
    embedding_df = None
    if generate_embeddings or not use_cached_embeddings:
        logger.info("\n" + "="*60)
        logger.info("Step 3: ç”Ÿæˆæ–‡æœ¬åµŒå…¥")
        logger.info("="*60)
        
        # åŠ è½½Redditå¸–å­
        reddit_posts = load_reddit_posts_for_embeddings(
            stock_symbol=stock_symbol,
            reddit_data_dir=reddit_data_dir
        )
        
        # ç”ŸæˆåµŒå…¥
        cache_file = os.path.join(output_dir, 'embeddings', 
                                 f'embeddings_{embedding_model.replace("/", "_")}.pkl')
        
        # åœ¨å¼€å§‹ç”ŸæˆåµŒå…¥å‰ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆå¹¶æå‰å‘ŠçŸ¥
        if not (use_cached_embeddings and os.path.exists(cache_file)):
            logger.info("\n" + "!"*60)
            logger.info("âš ï¸  é‡è¦æç¤ºï¼šå³å°†å¼€å§‹ç”Ÿæˆæ–‡æœ¬åµŒå…¥")
            logger.info("!"*60)
            logger.info(f"ğŸ“Š éœ€è¦å¤„ç†çš„å¸–å­æ•°é‡: {len(reddit_posts):,}")
            logger.info(f"ğŸ¤– ä½¿ç”¨çš„æ¨¡å‹: {embedding_model}")
            logger.info(f"â±ï¸  é¢„è®¡è€—æ—¶: 30åˆ†é’Ÿåˆ°æ•°å°æ—¶ï¼ˆå–å†³äºç¡¬ä»¶æ€§èƒ½ï¼‰")
            logger.info(f"ğŸ’¾ åµŒå…¥å°†ç¼“å­˜åˆ°: {cache_file}")
            logger.info("ğŸ“ å¼€å§‹ç”ŸæˆåµŒå…¥...")
            logger.info("!"*60 + "\n")
        
        if use_cached_embeddings and os.path.exists(cache_file):
            logger.info(f"ä½¿ç”¨ç¼“å­˜çš„åµŒå…¥: {cache_file}")
            try:
                embedding_df = pd.read_pickle(cache_file)
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½ç¼“å­˜ï¼Œé‡æ–°ç”Ÿæˆ: {e}")
                embedding_df = generate_embeddings_for_reddit_data(
                    reddit_df=reddit_posts,
                    model_name=embedding_model,
                    text_col='text_cleaned',
                    output_dir=os.path.join(output_dir, 'embeddings'),
                    cache_file=cache_file
                )
        else:
            embedding_df = generate_embeddings_for_reddit_data(
                reddit_df=reddit_posts,
                model_name=embedding_model,
                text_col='text_cleaned',
                output_dir=os.path.join(output_dir, 'embeddings'),
                cache_file=cache_file
            )
    
    # Step 4: èšåˆæ–‡æœ¬ç‰¹å¾
    if embedding_df is not None and not embedding_df.empty:
        logger.info("\n" + "="*60)
        logger.info("Step 4: èšåˆæ–‡æœ¬ç‰¹å¾")
        logger.info("="*60)
        
        aggregated_embeddings = aggregate_embeddings_by_hour(
            posts_df=embedding_df,
            timestamp_col='timestamp',
            embedding_prefix='embedding_',
            aggregation_method=aggregation_method,
            weight_col='score' if aggregation_method == 'weighted_mean' else None
        )
        
        # åˆå¹¶èšåˆåçš„åµŒå…¥ç‰¹å¾åˆ°åŸå§‹DataFrame
        # åªæå–åµŒå…¥åˆ—ï¼ˆä¸åŒ…æ‹¬Redditç»Ÿè®¡ï¼Œå› ä¸ºåŸå§‹dfå·²æœ‰ï¼‰
        embedding_cols = [col for col in aggregated_embeddings.columns if col.startswith('embedding_') or col == 'timestamp']
        embeddings_only = aggregated_embeddings[embedding_cols].copy()
        
        # åˆå¹¶åµŒå…¥ç‰¹å¾åˆ°åŸå§‹DataFrameï¼Œä¿ç•™æ‰€æœ‰åŸå§‹åˆ—ï¼ˆåŒ…æ‹¬è‚¡ç¥¨ä»·æ ¼æ•°æ®ï¼‰
        df = pd.merge(
            df,
            embeddings_only,
            on='timestamp',
            how='left'
        )
        
        # å¤„ç†ç¼ºå¤±å°æ—¶
        df = handle_missing_hours(
            df,
            timestamp_col='timestamp',
            embedding_prefix='embedding_',
            fill_method='zero'
        )
    
    # Step 5: è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
    logger.info("\n" + "="*60)
    logger.info("Step 5: è®¡ç®—æŠ€æœ¯æŒ‡æ ‡")
    logger.info("="*60)
    df = calculate_technical_indicators(df)
    
    # Step 6: å¤„ç†ç¼ºå¤±å€¼
    logger.info("\n" + "="*60)
    logger.info("Step 6: å¤„ç†ç¼ºå¤±å€¼")
    logger.info("="*60)
    
    # åˆ é™¤ç›®æ ‡å˜é‡ä¸ºNaNçš„è¡Œï¼ˆæœ€åä¸€è¡Œï¼‰
    initial_len = len(df)
    target_cols = [col for col in df.columns if col.startswith('target_')]
    if target_cols:
        df = df.dropna(subset=target_cols)
        logger.info(f"åˆ é™¤äº† {initial_len - len(df)} è¡Œï¼ˆç›®æ ‡å˜é‡ä¸ºNaNï¼‰")
    
    # å¡«å……å…¶ä»–ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨å‰å‘å¡«å……ï¼‰
    missing_before = df.isnull().sum().sum()
    if missing_before > 0:
        # å¯¹æ•°å€¼åˆ—ä½¿ç”¨å‰å‘å¡«å……
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].ffill().bfill().fillna(0)
        missing_after = df.isnull().sum().sum()
        logger.info(f"ç¼ºå¤±å€¼: {missing_before} -> {missing_after}")
    
    # Step 7: ä¿å­˜æœ€ç»ˆç‰¹å¾æ•°æ®é›†
    logger.info("\n" + "="*60)
    logger.info("Step 7: ä¿å­˜ç‰¹å¾æ•°æ®é›†")
    logger.info("="*60)
    
    output_path = os.path.join(output_dir, f'features_{stock_symbol}.csv')
    df.to_csv(output_path, index=False)
    logger.info(f"ç‰¹å¾æ•°æ®é›†å·²ä¿å­˜åˆ°: {output_path}")
    logger.info(f"æœ€ç»ˆç‰¹å¾æ•°é‡: {len(df.columns)}")
    logger.info(f"æœ€ç»ˆè®°å½•æ•°é‡: {len(df)}")
    
    # ç”Ÿæˆç‰¹å¾æŠ¥å‘Š
    generate_feature_report(df, stock_symbol, output_dir)
    
    return df


def generate_feature_report(df: pd.DataFrame, stock_symbol: str, output_dir: str):
    """
    ç”Ÿæˆç‰¹å¾å·¥ç¨‹æŠ¥å‘Š
    
    Args:
        df: ç‰¹å¾DataFrame
        stock_symbol: è‚¡ç¥¨ä»£ç 
        output_dir: è¾“å‡ºç›®å½•
    """
    report_path = os.path.join(output_dir, f'feature_report_{stock_symbol}.md')
    
    # ç»Ÿè®¡ç‰¹å¾ç±»å‹
    embedding_cols = [col for col in df.columns if col.startswith('embedding_')]
    reddit_cols = ['post_count', 'total_comments', 'total_score', 'unique_authors']
    target_cols = [col for col in df.columns if col.startswith('target_')]
    technical_cols = [col for col in df.columns if col not in 
                      embedding_cols + reddit_cols + target_cols + ['timestamp', 'stock_symbol', 
                                                                     'has_reddit_data', 'has_stock_data']]
    
    report = f"""# ç‰¹å¾å·¥ç¨‹æŠ¥å‘Š

**è‚¡ç¥¨ä»£ç **: {stock_symbol}
**ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## æ•°æ®æ¦‚è§ˆ

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ€»è®°å½•æ•° | {len(df):,} |
| æ€»ç‰¹å¾æ•° | {len(df.columns)} |
| æ—¶é—´èŒƒå›´ | {df['timestamp'].min()} è‡³ {df['timestamp'].max()} |

---

## ç‰¹å¾åˆ†ç±»

### 1. æ–‡æœ¬åµŒå…¥ç‰¹å¾
- **æ•°é‡**: {len(embedding_cols)}
- **ç»´åº¦**: {len(embedding_cols)} ç»´
- **è¯´æ˜**: ä½¿ç”¨sentence-transformersç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥å‘é‡

### 2. Redditç»Ÿè®¡ç‰¹å¾
- **æ•°é‡**: {len([col for col in reddit_cols if col in df.columns])}
- **ç‰¹å¾**: {', '.join([col for col in reddit_cols if col in df.columns])}

### 3. æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
- **æ•°é‡**: {len(technical_cols)}
- **ä¸»è¦ç±»å‹**: 
  - æ”¶ç›Šç‡ç‰¹å¾
  - ç§»åŠ¨å¹³å‡ç‰¹å¾
  - RSIã€MACDæŒ‡æ ‡
  - æ³¢åŠ¨ç‡ç‰¹å¾
  - æˆäº¤é‡ç‰¹å¾
  - æ»åç‰¹å¾
  - æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾

### 4. ç›®æ ‡å˜é‡
- **æ•°é‡**: {len(target_cols)}
- **å˜é‡**: {', '.join(target_cols)}

---

## ç‰¹å¾åˆ—è¡¨

### æ–‡æœ¬åµŒå…¥ç‰¹å¾
{chr(10).join([f'- {col}' for col in embedding_cols[:10]])}
... (å…± {len(embedding_cols)} ä¸ª)

### Redditç»Ÿè®¡ç‰¹å¾
{chr(10).join([f'- {col}' for col in reddit_cols if col in df.columns])}

### æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ï¼ˆç¤ºä¾‹ï¼‰
{chr(10).join([f'- {col}' for col in technical_cols[:20]])}
... (å…± {len(technical_cols)} ä¸ª)

---

## æ•°æ®è´¨é‡

| æ£€æŸ¥é¡¹ | ç»“æœ |
|--------|------|
| ç¼ºå¤±å€¼æ€»æ•° | {df.isnull().sum().sum()} |
| ç›®æ ‡å˜é‡ç¼ºå¤± | {df[target_cols].isnull().sum().sum() if target_cols else 0} |
| é‡å¤è®°å½• | {df.duplicated().sum()} |

---

## è¯´æ˜

- æ‰€æœ‰ç‰¹å¾å·²æŒ‰æ—¶é—´æ’åº
- ç›®æ ‡å˜é‡ä¸ºNaNçš„è®°å½•å·²åˆ é™¤
- å…¶ä»–ç¼ºå¤±å€¼å·²ä½¿ç”¨å‰å‘å¡«å……å¤„ç†
- æ–‡æœ¬åµŒå…¥ä½¿ç”¨ {embedding_model if 'embedding_model' in locals() else 'all-MiniLM-L6-v2'} æ¨¡å‹ç”Ÿæˆ
"""
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"ç‰¹å¾å·¥ç¨‹æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    import argparse
    
    parser = argparse.ArgumentParser(description='ç‰¹å¾å·¥ç¨‹ä¸»æµç¨‹')
    parser.add_argument('--symbol', type=str, default='GME', help='è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--embedding-model', type=str, default='all-MiniLM-L6-v2', 
                       help='åµŒå…¥æ¨¡å‹åç§°')
    parser.add_argument('--aggregation', type=str, default='mean', 
                       choices=['mean', 'weighted_mean', 'max'],
                       help='èšåˆæ–¹æ³•')
    parser.add_argument('--no-embeddings', action='store_true', 
                       help='è·³è¿‡åµŒå…¥ç”Ÿæˆï¼ˆä»…ä½¿ç”¨å·²æœ‰ç‰¹å¾ï¼‰')
    
    args = parser.parse_args()
    
    df = build_feature_pipeline(
        stock_symbol=args.symbol,
        embedding_model=args.embedding_model,
        aggregation_method=args.aggregation,
        generate_embeddings=not args.no_embeddings
    )
    
    logger.info("ç‰¹å¾å·¥ç¨‹æµç¨‹å®Œæˆï¼")


