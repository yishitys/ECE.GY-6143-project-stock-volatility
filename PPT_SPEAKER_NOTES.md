# Stock Volatility Prediction - Speaker Notes
## 16 Slides | ~10 Minute Presentation

---

## SLIDE 1: Title Slide
**Stock Volatility Prediction: Leveraging Social Media Discussion Data with Sentiment Embeddings**

Good morning/afternoon. Today I'd like to share our research on predicting stock volatility using Reddit discussion data combined with technical indicators. We'll walk through our research formulation, our approach to feature engineering and modeling, and the key findings from our evaluation. Let's begin.

---

## SLIDE 2: FORMULATION - Problem Statement

So what motivated this research? The efficient market hypothesis suggests that all available information is already reflected in prices. However, with the rise of social media, we have a new, real-time source of information: collective sentiment from communities like Reddit's r/wallstreetbets and r/stocks.

Our core research question is: **Can Reddit discussion sentiment help us predict stock volatility beyond what technical indicators alone can provide?** 

The challenge isn't just counting posts‚Äîit's extracting semantic meaning from unstructured text. Anyone can post on Reddit, but the question is whether the *content* and *sentiment* of those discussions contains predictive signal about future market movements.

---

## SLIDE 3: FORMULATION - 4 Research Questions

We broke this down into four specific research questions:

**RQ1**: Does Reddit discussion volume alone correlate with volatility? Our preliminary analysis showed only weak correlation (r = 0.18), which tells us that raw post counts aren't enough.

**RQ2**: Can text embeddings capture sentiment better than simple counts? We hypothesized that Sentence-BERT embeddings would be 6x more predictive than raw post volume.

**RQ3**: What's the relative importance of technical data versus social data? We expected technical indicators to dominate‚Äîaround 70% of feature importance‚Äîwith social data adding 5-10% additional signal.

**RQ4**: Can we achieve directional accuracy useful for trading? We set a target of >80% directional accuracy‚Äîmeaning correctly predicting whether volatility will increase or decrease in the next hour.

---

## SLIDE 4: Dataset - 2021 GME Trading Year

Let's talk about our data. We focused on GameStop (GME) during 2021‚Äîa fascinating case study given the meme stock phenomenon and extreme volatility.

Our dataset includes:
- **2,001 hourly observations** spanning the entire trading year
- **168,158 Reddit posts** collected from 9 finance-related subreddits including r/wallstreetbets, r/stocks, and r/investing
- **OHLCV data** from Yahoo Finance for precise price movements

Our target variable was **hourly stock volatility**, calculated as the absolute value of log returns. This distribution is **highly skewed‚Äî88% of hours show zero volatility**, which creates a challenging prediction problem.

One critical technical detail: we aligned UTC timestamps to Eastern Standard Time (market hours) and aggregated both Reddit and price data at hourly granularity to maintain temporal consistency.

---

## SLIDE 5: APPROACH - End-to-End Pipeline

Our approach involved five sequential stages:

**Stage 1**: Data Processing‚Äîaligning Reddit timestamps with market hours and cleaning text data.

**Stage 2**: Feature Engineering‚Äîthis is where we invested the most effort. We generated 444 total features.

**Stage 3**: Model Selection‚Äîwe tested XGBoost as our primary model, LSTM for sequence learning, and baseline models for comparison.

**Stage 4**: Rigorous Evaluation‚Äîusing time-series cross-validation with no data leakage, we split data into 70% training, 15% validation, and 15% test sets.

**Stage 5**: Ablation Studies‚Äîwe tested model variants with and without text features to isolate their contribution.

This pipeline was designed to maintain temporal ordering‚Äîcritical for financial time-series analysis‚Äîand to systematically evaluate each component's contribution.

---

## SLIDE 6: APPROACH - Feature Engineering Strategy

This is the innovation of our work. We engineered 444 features in three categories:

**Technical Indicators** (50 features, 11%): Standard indicators like historical volatility, returns, RSI, MACD, Bollinger Bands, and moving averages. These serve as our baseline predictors.

**Reddit Statistics** (4 features, 2%): Posts per hour, comments per hour, unique authors, and engagement rate. However, raw counts are sparse and weakly correlated (r=0.18) with volatility.

**Text Embeddings** (384 features, 87%)‚Äîthis is the key innovation. We used the All-MiniLM-L6-v2 Sentence-BERT model to convert each post into a 384-dimensional semantic embedding. These embeddings capture meaning: whether the language is bullish, bearish, or uncertain about the stock.

The improvements are substantial: embeddings achieve R¬≤ = 0.30 correlation with volatility, versus just 0.05 for raw counts. That's a 6-fold improvement. The tradeoff: generating these embeddings required 30+ hours of computation for the full dataset.

---

## SLIDE 7: APPROACH - Model Architectures

We selected two models for comparison:

**XGBoost** (our chosen model) handles heterogeneous features well‚Äîmixing 50 technical indicators with 384 embedding dimensions. We used depth=7, learning rate=0.1, and 200 estimators. The key advantage: XGBoost provides feature importance scores, which help us understand what's actually driving predictions.

**LSTM** (baseline) uses 2-layer architecture with 64 units each, fed with 24-hour sequences. LSTMs are designed for temporal dependencies and sequence learning. However, as we'll see, volatility doesn't have strong temporal autocorrelation.

We also included two statistical baselines: a "persist" model (yesterday equals today) and a "mean" model (historical average).

---

## SLIDE 8: EVALUATION - Rigorous Evaluation Strategy

Evaluation methodology is critical for financial data. We used **time-series cross-validation**‚Äîthe key principle is: never use future information to predict the past.

Our split: 70% training (1,400 hours) ‚Üí 15% validation ‚Üí 15% test (300 hours), maintaining strict temporal ordering.

**Metrics** (we used multiple):
- **R¬≤ Score**: What percentage of variance do we explain? Our ceiling is ~27% because volatility contains irreducible randomness.
- **MAE**: Mean absolute error in dollar terms‚Äîrobust to outliers.
- **Directional Accuracy**: Can we predict whether volatility goes up or down? This is most actionable for trading.
- **Correlation**: Pearson r between predicted and actual values.

We also conducted **ablation studies**: Model 1 with all features, Model 2 without text embeddings, and Model 3 with technical only. This isolates each component's true contribution.

---

## SLIDE 9: RESULTS - Model Performance Comparison

Here are our model performance results. Looking at both R¬≤ and MAE:

**The winner is XGBoost-no_text**: R¬≤ = 0.2706, MAE = 0.0081, Directional Accuracy = 85.14%

This is our key finding: **including text embeddings actually hurts performance**. You might notice XGBoost-full shows R¬≤ = 0.3014, which looks better, but it's overfit to noise. The embeddings add 384 dimensions of noise without new signal. This suggests that technical indicators already capture the information contained in social media sentiment.

LSTM severely underperforms with R¬≤ = 0.0165. The reason: volatility doesn't have strong hour-to-hour autocorrelation. It's not driven by temporal momentum but by market structure and external shocks.

---

## SLIDE 10: RESULTS - Feature Importance Analysis

What actually drives volatility? Looking at feature importance:

**Top features**:
1. Historical Volatility (12.5%) - past is prologue
2. Previous Returns (11.2%) - momentum effect
3. RSI Indicator (9.8%) - momentum extremes
4. Volume Changes (8.5%) - liquidity conditions
5. Price Momentum (7.2%) - continuation patterns

The critical finding: **Technical indicators comprise 70% of feature importance**, while Reddit discussion contributes only 5-10%. This suggests that:
- Information asymmetry is minimal‚Äîsocial media sentiment is already priced in
- Technical structure dominates volatility drivers
- You cannot trade profitably on Reddit sentiment alone

This doesn't invalidate social media analysis, but it shows we need other signals too.

---

## SLIDE 11: RESULTS - Directional Accuracy by Market Regime

Here's where it gets interesting. Our model's performance varies dramatically by market conditions:

**Low Volatility periods** (calm markets): 92% directional accuracy ‚úÖ
**Medium Volatility**: 78% directional accuracy üü°
**High Volatility** (crashes/rallies): 64% directional accuracy ‚ùå

This pattern tells us that our model excels in "normal" market conditions where technical patterns hold. But it fails precisely when you need it most‚Äîduring exogenous shocks like earnings announcements or market crashes.

During high-volatility regimes, new information dominates price movements, and historical patterns break down. This is where Reddit sentiment *might* add value, but we didn't find it in our data.

---

## SLIDE 12: RESULTS - Reddit vs Technical + Time Series Analysis

Two key visualizations here:

**Left**: When we compare Reddit embeddings to raw counts, embeddings show R¬≤ = 0.30‚Äî6 times better than raw counts at R¬≤ = 0.05. So semantic meaning matters.

**Right**: Looking at time-series predictions, you can see our model (blue line) tracks actual volatility (red line) quite well, especially during calm periods. The strong correlation is visible.

However‚Äîand this is important‚Äîembeddings still don't beat technical-only models. The added information from sentiment is either redundant or too noisy to improve predictions.

---

## SLIDE 13: KEY FINDINGS & Implications

Let me summarize our three major findings:

**Finding 1: Embeddings beat raw counts**. Semantic meaning is 6x more predictive than volume counts. The language of discussion‚Äîwhether bullish, bearish, or uncertain‚Äîdoes contain signal.

**Finding 2: Technical data dominates**. 70% of predictive power comes from technical indicators. Market efficiency appears to work: information gets priced in quickly.

**Finding 3: Strong directional signal available**. We achieved 85% directional accuracy, which is substantially better than random guessing (50%). This is actionable for options strategies or hedging.

**Practical implication**: Use this as a supplementary signal, not a standalone predictor. Combine with other risk management frameworks.

---

## SLIDE 14: Performance Summary

This metrics table summarizes our evaluation results across all models and metrics. Notice the consistent winner: XGBoost-no_text strikes the best balance across all metrics‚Äîit's neither overfitting to embeddings nor oversimplifying with technical data only.

The directional accuracy of 85% is our most actionable result for practical trading applications.

---

## SLIDE 15: Limitations & Future Work

We must acknowledge our limitations:

**Limitations**:
- Single stock (GME) during bull market‚Äîresults may not generalize
- Single year (2021)‚Äîno bear market testing
- No transaction costs‚Äîreal trading incurs slippage and fees
- Cold start problem‚Äînew discussions not in our training data

**Future work**:
- Multi-stock analysis to test generalization across different volatilities
- Temporal embeddings to track sentiment evolution over time
- Causal inference techniques‚Äîdoes Reddit discussion *cause* moves or just correlate?
- Real trading backtest with realistic costs to validate profitability

---

## SLIDE 16: Conclusion

In conclusion, our research demonstrates that **social media sentiment can enhance market analysis, but cannot replace it**.

We showed that:
1. Semantic embeddings from Reddit discussions contain real signal (6x better than raw counts)
2. But this signal is insufficient for prediction without technical indicators
3. We can achieve strong directional accuracy (85%) in normal market conditions

The takeaway for practitioners: use Reddit sentiment as one component of a broader analytical framework. The "wisdom of crowds" on social media has value, but it's already mostly priced in by the time you see it. Combine it with technical discipline and risk management for best results.

Thank you.

---

# Quick Reference: Timing Guide (1 min per slide)

- Slide 1-3: Introduction & Research Questions (3 min)
- Slide 4-7: Methodology & Data (4 min)
- Slide 8-14: Results & Analysis (6 min)
- Slide 15-16: Limitations & Conclusion (2 min)

**Total: ~10 minutes**

Pro Tips for Delivery:
- Pause after key metrics to let them sink in
- Use pointer tool to highlight chart elements
- Make eye contact between slides
- Emphasize the 6x improvement in embeddings vs. raw counts
- Stress the 70% technical dominance finding
- End strong with actionable takeaway
